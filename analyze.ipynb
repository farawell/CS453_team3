{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "da096733-8ea2-4f4a-a8f0-619cb12443e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time, pathlib, json, shelve\n",
    "from src import LLMFrontEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf3511-4540-4775-8009-70bdb13f9933",
   "metadata": {},
   "source": [
    "# Customize the following variables\n",
    "- eval_dir -- Path to the results \n",
    "- sample_dir -- Path to the system prompts\n",
    "- cache_file -- Path to a temporary file for caching the results from runs\n",
    "- cache_status -- Sets the cache status, true means use the cache\n",
    "- models -- List of models for which the results will be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2e618785-a01d-4f03-82cc-c5227a48e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = pathlib.Path(\"evals\", \"v3\")\n",
    "sample_dir = pathlib.Path(\"samples\")\n",
    "cache_file = pathlib.Path(\"evals\", \"cache\").as_posix()\n",
    "cache_status = True\n",
    "models = [\"gpt-4o-mini\", \"gemma2_9b\", \"qwen2.5_3b\", \"llama3.2_1b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313d56b-78fc-48a7-aba8-4a6ff2a84e4a",
   "metadata": {},
   "source": [
    "### Helper code for cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0a9ffe15-7d17-4467-a107-693025bf4fe3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def store_to_cache(list_key, value):\n",
    "    key = json.dumps(list_key)  # Convert list to JSON string for a key\n",
    "    with shelve.open(cache_file, writeback=True) as cache:\n",
    "        cache[key] = value  # Store value in cache\n",
    "\n",
    "def retrieve_from_cache(list_key):\n",
    "    if not cache_status:\n",
    "        return None\n",
    "\n",
    "    key = json.dumps(list_key)  # Convert list to JSON string for a key\n",
    "    with shelve.open(cache_file, writeback=False) as cache:\n",
    "        return cache.get(key, None)  # Retrieve value or return None if not found\n",
    "\n",
    "def enable_cache():\n",
    "    global cache_status\n",
    "    cache_status = True\n",
    "\n",
    "def disable_cache():\n",
    "    global cache_status\n",
    "    cache_status = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c952ba-30a9-4e4f-949d-18e3a26f5440",
   "metadata": {},
   "source": [
    "### Helper code for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3a6bcc6a-58ad-4702-b6a6-dfa7e9767074",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_system_prompt(system_prompt):\n",
    "    system_prompt_file = pathlib.Path(sample_dir, system_prompt, f\"{system_prompt}.prompty\")\n",
    "    if not system_prompt_file.exists():\n",
    "        return None\n",
    "    with open(system_prompt_file, \"r\") as f:\n",
    "        return f.read().split(\"system:\")[1].split(\"user:\")[0].strip()\n",
    "\n",
    "def check_test_result(result):\n",
    "    if \"ERR\" in result:\n",
    "        return \"ERR\"\n",
    "    else:\n",
    "        if \"OK\" not in result:\n",
    "            pass\n",
    "            # print warning in yellow that OK not found\n",
    "            # print(f\"\\033[93mWarning: OK not found in result\")\n",
    "            # reset color without newline\n",
    "            # print(\"\\033[0m\", end=\"\")\n",
    "        return \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "51151473-f0b6-46c4-909d-75924f5144f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_rules_grounded(system_prompt):\n",
    "    # print(f\"Checking {system_prompt}\")\n",
    "    rule_file = pathlib.Path(eval_dir, system_prompt, \"rules.txt\")\n",
    "    system_prompt_file = pathlib.Path(sample_dir, system_prompt, f\"{system_prompt}.prompty\")\n",
    "\n",
    "    if not system_prompt_file.exists():\n",
    "        print(f\"System prompt file not found: {system_prompt_file}\")\n",
    "        return\n",
    "\n",
    "    with open(system_prompt_file, \"r\") as f:\n",
    "        prompt = extract_system_prompt(system_prompt)\n",
    "        if not prompt:\n",
    "            return\n",
    "\n",
    "        with open(rule_file, \"r\") as f:\n",
    "            rules = f.read().splitlines()\n",
    "            total = len(rules)\n",
    "            grounded = 0\n",
    "\n",
    "            for rule in rules:\n",
    "                result = retrieve_from_cache([\"check_grounded\", rule, prompt])\n",
    "                if result is None:\n",
    "                    result = LLMFrontEnd().check_rule_grounded(rule, prompt)\n",
    "                    store_to_cache([\"check_grounded\", rule, prompt], result)\n",
    "\n",
    "                if result == \"0\":\n",
    "                    grounded += 1\n",
    "                # print(f\"Rule: {rule} Grounded: {result}\")\n",
    "            print(f\"Grounded: {grounded}/{total} ({grounded/total:.2f})\")\n",
    "\n",
    "def check_test_validity(system_prompt):\n",
    "    tests = categorize_test(system_prompt)\n",
    "\n",
    "    if not tests:\n",
    "        return\n",
    "\n",
    "    test_run_path = pathlib.Path(eval_dir, system_prompt, \"coverage\")\n",
    "    results = {\"promptpex\": {\"total\": 0, \"passed\": 0},\n",
    "              \"baseline\": {\"total\": 0, \"passed\": 0}}\n",
    "\n",
    "    for test_run_file in tests[\"valid\"]:\n",
    "        test_run_file = pathlib.Path(test_run_path, test_run_file)\n",
    "        if test_run_file.suffix == \".json\":\n",
    "            with open(test_run_file, \"r\") as f:\n",
    "                test_run = json.load(f)\n",
    "                if \"ruleid\" in test_run and test_run[\"ruleid\"] != None:\n",
    "                    results[\"promptpex\"][\"total\"] += 1\n",
    "                    results[\"promptpex\"][\"passed\"] += 1\n",
    "                else:\n",
    "                    results[\"baseline\"][\"total\"] += 1\n",
    "                    results[\"baseline\"][\"passed\"] += 1\n",
    "\n",
    "    for test_run_file in tests[\"invalid\"]:\n",
    "        test_run_file = pathlib.Path(test_run_path, test_run_file)\n",
    "        if test_run_file.suffix == \".json\":\n",
    "            with open(test_run_file, \"r\") as f:\n",
    "                test_run = json.load(f)\n",
    "                if \"ruleid\" in test_run and test_run[\"ruleid\"] != None:\n",
    "                    results[\"promptpex\"][\"total\"] += 1\n",
    "                else:\n",
    "                    results[\"baseline\"][\"total\"] += 1\n",
    "\n",
    "    promptpex_total = results[\"promptpex\"][\"total\"]\n",
    "    promptpex_passed = results[\"promptpex\"][\"passed\"]\n",
    "    baseline_total = results[\"baseline\"][\"total\"]\n",
    "    baseline_passed = results[\"baseline\"][\"passed\"]\n",
    "\n",
    "    print(f\"Valid:\\t\\t{promptpex_passed}/{promptpex_total}\", end=\"\")\n",
    "    if promptpex_total == 0:\n",
    "        print(\"\\t\", end=\"\")\n",
    "    else:\n",
    "        print(f\" ({promptpex_passed/promptpex_total:.2f})\", end=\"\")\n",
    "    print(f\"\\t {baseline_passed}/{baseline_total} \", end=\"\")\n",
    "    if baseline_total == 0:\n",
    "        print(\"\\t\")\n",
    "    else:\n",
    "        print(f\" ({baseline_passed/baseline_total:.2f})\")\n",
    "\n",
    "def categorize_test(system_prompt):\n",
    "    tests = {\"valid\": [], \"invalid\": []}\n",
    "\n",
    "    test_run_path = pathlib.Path(eval_dir, system_prompt, \"coverage\")\n",
    "    system_prompt = extract_system_prompt(system_prompt)\n",
    "\n",
    "    if not system_prompt:\n",
    "        return None\n",
    "\n",
    "    for test_run_file in test_run_path.iterdir():\n",
    "        if test_run_file.suffix == \".json\":\n",
    "            with open(test_run_file, \"r\") as f:\n",
    "                test_run = json.load(f)\n",
    "                if \"input\" in test_run:\n",
    "                    result = retrieve_from_cache([\"categorize_test\", test_run[\"input\"], system_prompt])\n",
    "                    if result is None:\n",
    "                        result = LLMFrontEnd().check_violation_with_input_spec(test_run[\"input\"], system_prompt)\n",
    "                        store_to_cache([\"categorize_test\", test_run[\"input\"], system_prompt], result)\n",
    "                    test_file_name = test_run_file.parts[-1]\n",
    "                    if check_test_result(result) == \"OK\":\n",
    "                        tests[\"valid\"].append(test_file_name)\n",
    "                    else:\n",
    "                        tests[\"invalid\"].append(test_file_name)\n",
    "    return tests\n",
    "\n",
    "def check_coverage(system_prompt):\n",
    "    test_run_path = pathlib.Path(eval_dir, system_prompt, \"coverage\")\n",
    "    # loop over all the json files in test_run_path\n",
    "    total = 0\n",
    "    passed = 0\n",
    "\n",
    "    valid_tests = categorize_test(system_prompt)\n",
    "    if not valid_tests:\n",
    "        return\n",
    "    valid_tests = valid_tests[\"valid\"]\n",
    "\n",
    "    for test_run_file in valid_tests:\n",
    "        test_run_file = pathlib.Path(test_run_path, test_run_file)\n",
    "        if test_run_file.suffix == \".json\":\n",
    "            with open(test_run_file, \"r\") as f:\n",
    "                test_run = json.load(f)\n",
    "                if \"evaluation\" in test_run:\n",
    "                    total += 1\n",
    "                    result = check_test_result(test_run[\"evaluation\"])\n",
    "                    if result == \"OK\":\n",
    "                        passed += 1\n",
    "    print(f\"Coverage: {passed}/{total} ({passed/total:.2f})\")\n",
    "\n",
    "def check_failure(system_prompt):\n",
    "    # list of colors for printing\n",
    "    colors = [\"\\033[91m\", \"\\033[93m\", \"\\033[94m\", \"\\033[95m\", \"\\033[96m\", \"\\033[97m\", \"\\033[98m\"]\n",
    "    for model in models:\n",
    "        test_run_path = pathlib.Path(eval_dir, system_prompt, model)\n",
    "        if not test_run_path.exists():\n",
    "            continue\n",
    "        # loop over all the json files in test_run_path\n",
    "        results = {\"promptpex\": {\"total\": 0, \"passed\": 0}, \"baseline\": {\"total\": 0, \"passed\": 0}}\n",
    "        for test_run_file in test_run_path.iterdir():\n",
    "            if test_run_file.suffix == \".json\":\n",
    "                with open(test_run_file, \"r\") as f:\n",
    "                    test_run = json.load(f)\n",
    "                    if \"evaluation\" in test_run:\n",
    "                        result = check_test_result(test_run[\"evaluation\"])\n",
    "                        if \"ruleid\" in test_run and test_run[\"ruleid\"] != None:\n",
    "                            results[\"promptpex\"][\"total\"] += 1\n",
    "                            if result == \"OK\":\n",
    "                                results[\"promptpex\"][\"passed\"] += 1\n",
    "                        else:\n",
    "                            results[\"baseline\"][\"total\"] += 1\n",
    "                            if result == \"OK\":\n",
    "                                results[\"baseline\"][\"passed\"] += 1\n",
    "\n",
    "        promptpex_total = results[\"promptpex\"][\"total\"]\n",
    "        promptpex_passed = results[\"promptpex\"][\"passed\"]\n",
    "        baseline_total =  results[\"baseline\"][\"total\"]\n",
    "        baseline_passed = results[\"baseline\"][\"passed\"]\n",
    "\n",
    "        # print in color using the model index\n",
    "        print(f\"{colors[models.index(model)]}{model}:\\t{promptpex_passed}/{promptpex_total}\", end=\"\")\n",
    "        if promptpex_total == 0:\n",
    "            print(\"\\t\", end=\"\")\n",
    "        else:\n",
    "            print(f\" ({promptpex_passed/promptpex_total:.2f})\", end=\"\")\n",
    "        print(f\"\\t {baseline_passed}/{baseline_total} \", end=\"\")\n",
    "        if baseline_total == 0:\n",
    "            print(\"\\t\")\n",
    "        else:\n",
    "            print(f\" ({baseline_passed/baseline_total:.2f})\")\n",
    "        print(\"\\033[0m\", end=\"\")\n",
    "\n",
    "        valid_results = {\"promptpex\": {\"total\": 0, \"passed\": 0}, \"baseline\": {\"total\": 0, \"passed\": 0}}\n",
    "        tests = categorize_test(system_prompt)\n",
    "        if not tests:\n",
    "            continue\n",
    "\n",
    "        for test_run_file in tests[\"valid\"]:\n",
    "            test_run_file = pathlib.Path(test_run_path, test_run_file)\n",
    "            if test_run_file.suffix == \".json\":\n",
    "                with open(test_run_file, \"r\") as f:\n",
    "                    test_run = json.load(f)\n",
    "                    if \"evaluation\" in test_run:\n",
    "                        result = check_test_result(test_run[\"evaluation\"])\n",
    "                        if \"ruleid\" in test_run and test_run[\"ruleid\"] != None:\n",
    "                            valid_results[\"promptpex\"][\"total\"] += 1\n",
    "                            if result == \"OK\":\n",
    "                                valid_results[\"promptpex\"][\"passed\"] += 1\n",
    "                        else:\n",
    "                            valid_results[\"baseline\"][\"total\"] += 1\n",
    "                            if result == \"OK\":\n",
    "                                valid_results[\"baseline\"][\"passed\"] += 1\n",
    "\n",
    "        valid_promptpex_total = valid_results[\"promptpex\"][\"total\"]\n",
    "        valid_promptpex_passed = valid_results[\"promptpex\"][\"passed\"]\n",
    "        valid_baseline_total =  valid_results[\"baseline\"][\"total\"]\n",
    "        valid_baseline_passed = valid_results[\"baseline\"][\"passed\"]\n",
    "\n",
    "        # print in color using the model index\n",
    "        print(f\"{colors[models.index(model)]}V-{model}:\\t{valid_promptpex_passed}/{valid_promptpex_total}\", end=\"\")\n",
    "        if valid_promptpex_total == 0:\n",
    "            print(\"\\t\", end=\"\")\n",
    "        else:\n",
    "            print(f\" ({valid_promptpex_passed/valid_promptpex_total:.2f})\", end=\"\")\n",
    "        print(f\"\\t {valid_baseline_passed}/{valid_baseline_total} \", end=\"\")\n",
    "        if valid_baseline_total == 0:\n",
    "            print(\"\\t\")\n",
    "        else:\n",
    "            print(f\" ({valid_baseline_passed/valid_baseline_total:.2f})\")\n",
    "        print(\"\\033[0m\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b8caccdb-e121-45c7-a389-b03faf7684bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_help():\n",
    "    print(f\"\\033[92mProcessing Prompt Name\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"\\033[0m\", end=\"\")\n",
    "    data = \"\"\"Grounded: Number of grounded rules/Total Rules\n",
    "Coverage: Number of passed tests in the coverage run/Total test runs (uses all the valid tests) \n",
    "\\t\\tPromptPex\\t\\t Baseline\n",
    "Valid: Number of valid tests/total Test\n",
    "Model Name: Number of passed tests for this model/total test runs\n",
    "V-Model Name: Number of passed tests for this model/valid test runs\n",
    "\"\"\"\n",
    "    print(data)\n",
    "\n",
    "def print_invalid_tests(system_prompt):\n",
    "    tests = categorize_test(system_prompt)\n",
    "    if not tests:\n",
    "        return\n",
    "    print(\"Invalid tests:\")\n",
    "    test_run_path = pathlib.Path(eval_dir, system_prompt, \"coverage\")\n",
    "    for test_run_file in tests[\"invalid\"]:\n",
    "        test_run_file = pathlib.Path(test_run_path, test_run_file)\n",
    "        if test_run_file.suffix == \".json\":\n",
    "            with open(test_run_file, \"r\") as f:\n",
    "                test_run = json.load(f)\n",
    "                if \"ruleid\" in test_run and test_run[\"ruleid\"] != None:\n",
    "                    print(\"promptpex: \", end=\"\")\n",
    "                else:\n",
    "                    print(\"baseline: \", end=\"\")\n",
    "                print(f\"{test_run['input']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a6cf1de7-c7ba-4e2f-bc2d-00f3ba008d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mProcessing Prompt Name\n",
      "----------------------------------------\n",
      "\u001b[0mGrounded: Number of grounded rules/Total Rules\n",
      "Coverage: Number of passed tests in the coverage run/Total test runs (uses all the valid tests) \n",
      "\t\tPromptPex\t\t Baseline\n",
      "Valid: Number of valid tests/total Test\n",
      "Model Name: Number of passed tests for this model/total test runs\n",
      "V-Model Name: Number of passed tests for this model/valid test runs\n",
      "\n",
      "\n",
      "\u001b[92mProcessing speech-tag\n",
      "----------------------------------------\n",
      "\u001b[0mGrounded: 5/5 (1.00)\n",
      "Coverage: 42/42 (1.00)\n",
      "\t\t PromptPex \t Baseline\n",
      "Valid:\t\t23/30 (0.77)\t 19/20  (0.95)\n",
      "\u001b[91mgpt-4o-mini:\t30/30 (1.00)\t 20/20  (1.00)\n",
      "\u001b[0m\u001b[91mV-gpt-4o-mini:\t23/23 (1.00)\t 19/19  (1.00)\n",
      "\u001b[0m\u001b[93mgemma2_9b:\t25/30 (0.83)\t 20/20  (1.00)\n",
      "\u001b[0m\u001b[93mV-gemma2_9b:\t19/23 (0.83)\t 19/19  (1.00)\n",
      "\u001b[0m\u001b[94mqwen2.5_3b:\t28/30 (0.93)\t 18/20  (0.90)\n",
      "\u001b[0m\u001b[94mV-qwen2.5_3b:\t21/23 (0.91)\t 17/19  (0.89)\n",
      "\u001b[0m\u001b[95mllama3.2_1b:\t2/30 (0.07)\t 2/20  (0.10)\n",
      "\u001b[0m\u001b[95mV-llama3.2_1b:\t2/23 (0.09)\t 2/19  (0.11)\n",
      "\u001b[0m\n",
      "\u001b[92mProcessing text-to-p\n",
      "----------------------------------------\n",
      "\u001b[0mGrounded: 9/9 (1.00)\n",
      "Coverage: 59/59 (1.00)\n",
      "\t\t PromptPex \t Baseline\n",
      "Valid:\t\t54/54 (1.00)\t 5/5  (1.00)\n",
      "\u001b[91mgpt-4o-mini:\t48/54 (0.89)\t 5/5  (1.00)\n",
      "\u001b[0m\u001b[91mV-gpt-4o-mini:\t48/54 (0.89)\t 5/5  (1.00)\n",
      "\u001b[0m\u001b[93mgemma2_9b:\t44/54 (0.81)\t 3/5  (0.60)\n",
      "\u001b[0m\u001b[93mV-gemma2_9b:\t44/54 (0.81)\t 3/5  (0.60)\n",
      "\u001b[0m\u001b[94mqwen2.5_3b:\t18/54 (0.33)\t 3/5  (0.60)\n",
      "\u001b[0m\u001b[94mV-qwen2.5_3b:\t18/54 (0.33)\t 3/5  (0.60)\n",
      "\u001b[0m\u001b[95mllama3.2_1b:\t9/54 (0.17)\t 1/5  (0.20)\n",
      "\u001b[0m\u001b[95mV-llama3.2_1b:\t9/54 (0.17)\t 1/5  (0.20)\n",
      "\u001b[0m\n",
      "\u001b[92mProcessing elements\n",
      "----------------------------------------\n",
      "\u001b[0mSystem prompt file not found: samples/elements/elements.prompty\n",
      "\t\t PromptPex \t Baseline\n",
      "\u001b[91mgpt-4o-mini:\t27/36 (0.75)\t 10/10  (1.00)\n",
      "\u001b[0m\u001b[93mgemma2_9b:\t27/36 (0.75)\t 6/10  (0.60)\n",
      "\u001b[0m\u001b[94mqwen2.5_3b:\t15/36 (0.42)\t 10/10  (1.00)\n",
      "\u001b[0m\u001b[95mllama3.2_1b:\t12/36 (0.33)\t 3/10  (0.30)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "enable_cache()\n",
    "print_help()\n",
    "for sample in eval_dir.iterdir():\n",
    "    if sample.is_dir():\n",
    "        # print in green\n",
    "        print(\"\")\n",
    "        print(f\"\\033[92mProcessing {sample.parts[-1]}\")\n",
    "        print(\"-\" * 40)\n",
    "        # reset color without newline\n",
    "        print(\"\\033[0m\", end=\"\")\n",
    "        # get name of the last dir in the path\n",
    "        system_prompt = sample.parts[-1]\n",
    "\n",
    "        \n",
    "        check_rules_grounded(system_prompt)\n",
    "        check_coverage(system_prompt)\n",
    "        print(f\"\\t\\t PromptPex \\t Baseline\")\n",
    "        check_test_validity(system_prompt)\n",
    "        check_failure(system_prompt)\n",
    "\n",
    "        # print_invalid_tests(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb2c0c-571a-44a9-b35d-de77e1614c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
