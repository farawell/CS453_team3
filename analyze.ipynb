{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da096733-8ea2-4f4a-a8f0-619cb12443e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time, pathlib, json, shelve\n",
    "from src import LLMFrontEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e618785-a01d-4f03-82cc-c5227a48e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = pathlib.Path(\"evals\", \"v0\")\n",
    "sample_dir = pathlib.Path(\"samples\")\n",
    "cache_file = pathlib.Path(\"evals\", \"cache\").as_posix()\n",
    "cache_status = True\n",
    "models = [\"gpt-4o-mini\", \"gemma2_9b\", \"qwen2.5_3b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9ffe15-7d17-4467-a107-693025bf4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_cache(list_key, value):\n",
    "    key = json.dumps(list_key)  # Convert list to JSON string for a key\n",
    "    with shelve.open(cache_file, writeback=True) as cache:\n",
    "        cache[key] = value  # Store value in cache\n",
    "\n",
    "def retrieve_from_cache(list_key):\n",
    "    if not cache_status:\n",
    "        return None\n",
    "\n",
    "    key = json.dumps(list_key)  # Convert list to JSON string for a key\n",
    "    with shelve.open(cache_file, writeback=False) as cache:\n",
    "        return cache.get(key, None)  # Retrieve value or return None if not found\n",
    "\n",
    "def enable_cache():\n",
    "    global cache_status\n",
    "    cache_status = True\n",
    "\n",
    "def disable_cache():\n",
    "    global cache_status\n",
    "    cache_status = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a6bcc6a-58ad-4702-b6a6-dfa7e9767074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_system_prompt(system_prompt):\n",
    "    system_prompt_file = pathlib.Path(sample_dir, system_prompt, f\"{system_prompt}.prompty\")\n",
    "    with open(system_prompt_file, \"r\") as f:\n",
    "        return f.read().split(\"system:\")[1].split(\"user:\")[0].strip()\n",
    "\n",
    "def check_test_result(result):\n",
    "    if \"ERR\" in result:\n",
    "        return \"ERR\"\n",
    "    else:\n",
    "        if \"OK\" not in result:\n",
    "            pass\n",
    "            # print warning in yellow that OK not found\n",
    "            # print(f\"\\033[93mWarning: OK not found in result\")\n",
    "            # reset color without newline\n",
    "            # print(\"\\033[0m\", end=\"\")\n",
    "        return \"OK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51151473-f0b6-46c4-909d-75924f5144f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rules_grounded(system_prompt):\n",
    "    # print(f\"Checking {system_prompt}\")\n",
    "    rule_file = pathlib.Path(eval_dir, system_prompt, \"rules.txt\")\n",
    "    system_prompt_file = pathlib.Path(sample_dir, system_prompt, f\"{system_prompt}.prompty\")\n",
    "\n",
    "    if not system_prompt_file.exists():\n",
    "        print(f\"System prompt file not found: {system_prompt_file}\")\n",
    "        return\n",
    "\n",
    "    with open(system_prompt_file, \"r\") as f:\n",
    "        prompt = extract_system_prompt(system_prompt)\n",
    "\n",
    "        with open(rule_file, \"r\") as f:\n",
    "            rules = f.read().splitlines()\n",
    "            total = len(rules)\n",
    "            grounded = 0\n",
    "\n",
    "            for rule in rules:\n",
    "                result = retrieve_from_cache([\"check_grounded\", rule, prompt])\n",
    "                if result is None:\n",
    "                    result = LLMFrontEnd().check_rule_grounded(rule, prompt)\n",
    "                    store_to_cache([\"check_grounded\", rule, prompt], result)\n",
    "\n",
    "                if result == \"0\":\n",
    "                    grounded += 1\n",
    "                # print(f\"Rule: {rule} Grounded: {result}\")\n",
    "            print(f\"Grounded: {grounded}/{total} ({grounded/total:.2f})\")\n",
    "\n",
    "def check_test_validity(system_prompt):\n",
    "    # assumes coverage has all the tests\n",
    "    test_run_path = pathlib.Path(eval_dir, system_prompt, \"coverage\")\n",
    "    total = 0\n",
    "    passed = 0\n",
    "\n",
    "    system_prompt_file = pathlib.Path(sample_dir, system_prompt, f\"{system_prompt}.prompty\")\n",
    "    input_spec_file = pathlib.Path(eval_dir, system_prompt, \"input_spec.txt\")\n",
    "\n",
    "    if not input_spec_file.exists():\n",
    "        print(f\"Input spec file not found: {input_spec_file}\")\n",
    "        return\n",
    "\n",
    "    with open(input_spec_file, \"r\") as f:\n",
    "        input_spec = f.read()\n",
    "\n",
    "        for test_run_file in test_run_path.iterdir():\n",
    "            if test_run_file.suffix == \".json\":\n",
    "                with open(test_run_file, \"r\") as f:\n",
    "                    test_run = json.load(f)\n",
    "                    if \"input\" in test_run:\n",
    "                        total += 1\n",
    "                        result = retrieve_from_cache([\"check_violation\", test_run[\"input\"], input_spec])\n",
    "                        if result is None:\n",
    "                            result = LLMFrontEnd().check_violation_with_input_spec(test_run[\"input\"], input_spec)\n",
    "                            store_to_cache([\"check_violation\", test_run[\"input\"], input_spec], result)\n",
    "                        if result == \"0\":\n",
    "                            passed += 1\n",
    "    print(f\"Valid: {passed}/{total} ({passed/total:.2f})\")\n",
    "    \n",
    "def check_coverage(system_prompt):\n",
    "    test_run_path = pathlib.Path(eval_dir, system_prompt, \"coverage\")\n",
    "    # loop over all the json files in test_run_path\n",
    "    total = 0\n",
    "    passed = 0\n",
    "    for test_run_file in test_run_path.iterdir():\n",
    "        if test_run_file.suffix == \".json\":\n",
    "            with open(test_run_file, \"r\") as f:\n",
    "                test_run = json.load(f)\n",
    "                if \"evaluation\" in test_run:\n",
    "                    total += 1\n",
    "                    result = check_test_result(test_run[\"evaluation\"])\n",
    "                    if result == \"OK\":\n",
    "                        passed += 1\n",
    "    print(f\"Coverage: {passed}/{total} ({passed/total:.2f})\")\n",
    "\n",
    "def check_failure(system_prompt):\n",
    "    # list of colors for printing\n",
    "    colors = [\"\\033[91m\", \"\\033[93m\", \"\\033[94m\", \"\\033[95m\", \"\\033[96m\", \"\\033[97m\", \"\\033[98m\"]\n",
    "    for model in models:\n",
    "        test_run_path = pathlib.Path(eval_dir, system_prompt, model)\n",
    "        # loop over all the json files in test_run_path\n",
    "        total = 0\n",
    "        passed = 0\n",
    "        for test_run_file in test_run_path.iterdir():\n",
    "            if test_run_file.suffix == \".json\":\n",
    "                with open(test_run_file, \"r\") as f:\n",
    "                    test_run = json.load(f)\n",
    "                    if \"evaluation\" in test_run:\n",
    "                        total += 1\n",
    "                        result = check_test_result(test_run[\"evaluation\"])\n",
    "                        if result == \"OK\":\n",
    "                            passed += 1\n",
    "        # print in color using the model index\n",
    "        print(f\"{colors[models.index(model)]}{model}: {passed}/{total} ({passed/total:.2f})\")\n",
    "        # reset color without newline\n",
    "        print(\"\\033[0m\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6cf1de7-c7ba-4e2f-bc2d-00f3ba008d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92mProcessing elements\n",
      "----------------------------------------\n",
      "\u001b[0mSystem prompt file not found: samples/elements/elements.prompty\n",
      "Valid: 7/9 (0.78)\n",
      "Coverage: 9/9 (1.00)\n",
      "\u001b[91mgpt-4o-mini: 8/9 (0.89)\n",
      "\u001b[0m\u001b[93mgemma2_9b: 7/9 (0.78)\n",
      "\u001b[0m\u001b[94mqwen2.5_3b: 4/9 (0.44)\n",
      "\u001b[0m\n",
      "\u001b[92mProcessing speech-tag\n",
      "----------------------------------------\n",
      "\u001b[0mGrounded: 4/4 (1.00)\n",
      "Valid: 10/12 (0.83)\n",
      "Coverage: 12/12 (1.00)\n",
      "\u001b[91mgpt-4o-mini: 12/12 (1.00)\n",
      "\u001b[0m\u001b[93mgemma2_9b: 11/12 (0.92)\n",
      "\u001b[0m\u001b[94mqwen2.5_3b: 11/12 (0.92)\n",
      "\u001b[0m\n",
      "\u001b[92mProcessing text-to-p\n",
      "----------------------------------------\n",
      "\u001b[0mGrounded: 5/6 (0.83)\n",
      "Valid: 3/3 (1.00)\n",
      "Coverage: 3/3 (1.00)\n",
      "\u001b[91mgpt-4o-mini: 0/3 (0.00)\n",
      "\u001b[0m\u001b[93mgemma2_9b: 1/3 (0.33)\n",
      "\u001b[0m\u001b[94mqwen2.5_3b: 0/3 (0.00)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "enable_cache()\n",
    "for sample in eval_dir.iterdir():\n",
    "    if sample.is_dir():\n",
    "        # print in green\n",
    "        print(\"\")\n",
    "        print(f\"\\033[92mProcessing {sample.parts[-1]}\")\n",
    "        print(\"-\" * 40)\n",
    "        # reset color without newline\n",
    "        print(\"\\033[0m\", end=\"\")\n",
    "        # get name of the last dir in the path\n",
    "        system_prompt = sample.parts[-1]\n",
    "\n",
    "        check_rules_grounded(system_prompt)\n",
    "        check_test_validity(system_prompt)\n",
    "        check_coverage(system_prompt)\n",
    "        check_failure(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a9d8e-16cb-4845-8a46-e6abd6e518ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
